{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f87dd5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok\n"
     ]
    }
   ],
   "source": [
    "print(\"ok\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9128d207",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Creating FAISS index...\n",
      "üìñ Loaded 637 pages\n",
      "‚úÇÔ∏è Created 6600 text chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/nl/q3f58lb91kl7sqnyg6y9r4dr0000gn/T/ipykernel_31102/3351946085.py:22: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
      "/Users/apple/miniconda3/envs/medibot/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ FAISS index created at '../faiss_index'\n"
     ]
    }
   ],
   "source": [
    "# --- ADD THIS CELL TO YOUR NOTEBOOK ---\n",
    "import os\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "print(\"üöÄ Creating FAISS index...\")\n",
    "\n",
    "# Load and process the PDF\n",
    "file_path = \"../Data/Medical_book.pdf\"  # Note the ../ to go up one level\n",
    "loader = PyPDFLoader(file_path)\n",
    "docs = loader.load()\n",
    "print(f\"üìñ Loaded {len(docs)} pages\")\n",
    "\n",
    "# Split into chunks\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\n",
    "chunks = splitter.split_documents(docs)\n",
    "print(f\"‚úÇÔ∏è Created {len(chunks)} text chunks\")\n",
    "\n",
    "# Create embeddings and FAISS index\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "vectorstore = FAISS.from_documents(chunks, embeddings)\n",
    "\n",
    "# Save the index (saves to main project folder)\n",
    "vectorstore.save_local(\"../faiss_index\")\n",
    "print(\"‚úÖ FAISS index created at '../faiss_index'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8adb7bd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/apple/Desktop/End-to-end-Medical-Chatbot-Generative-AI/research'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "538ed7b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "552ca0d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/apple/Desktop/End-to-end-Medical-Chatbot-Generative-AI'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "31f07589",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader, DirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a54b86a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract Data From the PDF File\n",
    "def load_pdf_file(data):\n",
    "    loader= DirectoryLoader(data,\n",
    "                            glob=\"*.pdf\",\n",
    "                            loader_cls=PyPDFLoader)\n",
    "\n",
    "    documents=loader.load()\n",
    "\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9023de0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_data=load_pdf_file(data='Data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d0c199fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a60f678d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split the Data into Text Chunks\n",
    "def text_split(extracted_data):\n",
    "    text_splitter=RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=20)\n",
    "    text_chunks=text_splitter.split_documents(extracted_data)\n",
    "    return text_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "230c5eaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of Text Chunks 5860\n"
     ]
    }
   ],
   "source": [
    "text_chunks=text_split(extracted_data)\n",
    "print(\"Length of Text Chunks\", len(text_chunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c00bad3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "# Replace with the actual path to your PDF file\n",
    "loader = PyPDFLoader(\"/Users/apple/Desktop/End-to-end-Medical-Chatbot-Generative-AI/Data/Medical_book.pdf\")  # Update this with your file's location\n",
    "extracted_data = loader.load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0eb48e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c642865b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Download the Embeddings from Hugging Face\n",
    "def download_hugging_face_embeddings():\n",
    "    embeddings=HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2')\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f9ee7c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from langchain_huggingface import HuggingFaceEmbeddings # type: ignore\n",
    "except ImportError:\n",
    "    from langchain.embeddings import HuggingFaceEmbeddings  # fallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "adb544a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from langchain_huggingface import HuggingFaceEmbeddings # type: ignore\n",
    "except ImportError:\n",
    "    from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e85004c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length 384\n"
     ]
    }
   ],
   "source": [
    "query_result = embeddings.embed_query(\"Hello world\")\n",
    "print(\"Length\", len(query_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c36fe1ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c97219e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "PINECONE_API_KEY=os.environ.get('PINECONE_API_KEY')\n",
    "OPENAI_API_KEY=os.environ.get('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d5351f8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index 'medicalbot' already exists.\n"
     ]
    }
   ],
   "source": [
    "from pinecone.grpc import PineconeGRPC as Pinecone\n",
    "from pinecone import ServerlessSpec\n",
    "import os\n",
    "\n",
    "PINECONE_API_KEY = os.environ.get(\"PINECONE_API_KEY\")\n",
    "pc = Pinecone(api_key=PINECONE_API_KEY)\n",
    "\n",
    "index_name = \"medicalbot\"\n",
    "\n",
    "# Check if index already exists\n",
    "if index_name not in pc.list_indexes().names():\n",
    "    pc.create_index(\n",
    "        name=index_name,\n",
    "        dimension=384,\n",
    "        metric=\"cosine\",\n",
    "        spec=ServerlessSpec(\n",
    "            cloud=\"aws\",\n",
    "            region=\"us-east-1\"\n",
    "        )\n",
    "    )\n",
    "else:\n",
    "    print(f\"Index '{index_name}' already exists.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f1d26169",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"PINECONE_API_KEY\"] = PINECONE_API_KEY\n",
    "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a905effb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embed each chunk and upsert the embeddings into your Pinecone index.\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "\n",
    "docsearch = PineconeVectorStore.from_documents(\n",
    "    documents=text_chunks,\n",
    "    index_name=index_name,\n",
    "    embedding=embeddings, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4c3295aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Existing index \n",
    "\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "# Embed each chunk and upsert the embeddings into your Pinecone index.\n",
    "docsearch = PineconeVectorStore.from_existing_index(\n",
    "    index_name=index_name,\n",
    "    embedding=embeddings\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ebfb5dda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_pinecone.vectorstores.PineconeVectorStore at 0x36aa6fd60>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docsearch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ca0ac6be",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = docsearch.as_retriever(search_type=\"similarity\", search_kwargs={\"k\":3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f75ef86c",
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieved_docs = retriever.invoke(\"What is Acne?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "edbad6c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='9b5d3509-6416-4954-bba7-bf288572503d', metadata={'creationdate': '2004-12-18T17:00:02-05:00', 'creator': 'PyPDF', 'moddate': '2004-12-18T16:15:31-06:00', 'page': 39.0, 'page_label': '40', 'producer': 'PDFlib+PDI 5.0.0 (SunOS)', 'source': '/Users/apple/Desktop/End-to-end-Medical-Chatbot-Generative-AI/Data/Medical_book.pdf', 'total_pages': 637.0}, page_content='GALE ENCYCLOPEDIA OF MEDICINE 226\\nAcne\\nGEM - 0001 to 0432 - A  10/22/03 1:41 PM  Page 26'),\n",
       " Document(id='c6f9c47f-7d40-4eda-b85a-35f21f405895', metadata={'creationdate': '2004-12-18T17:00:02-05:00', 'creator': 'PyPDF', 'moddate': '2004-12-18T16:15:31-06:00', 'page': 39.0, 'page_label': '40', 'producer': 'PDFlib+PDI 5.0.0 (SunOS)', 'source': 'Data/Medical_book.pdf', 'total_pages': 637.0}, page_content='GALE ENCYCLOPEDIA OF MEDICINE 226\\nAcne\\nGEM - 0001 to 0432 - A  10/22/03 1:41 PM  Page 26'),\n",
       " Document(id='6a2de801-bed3-4cf4-8067-b7e1099bc9c4', metadata={'creationdate': '2004-12-18T17:00:02-05:00', 'creator': 'PyPDF', 'moddate': '2004-12-18T16:15:31-06:00', 'page': 39.0, 'page_label': '40', 'producer': 'PDFlib+PDI 5.0.0 (SunOS)', 'source': '/Users/apple/Desktop/End-to-end-Medical-Chatbot-Generative-AI/Data/Medical_book.pdf', 'total_pages': 637.0}, page_content='GALE ENCYCLOPEDIA OF MEDICINE 226\\nAcne\\nGEM - 0001 to 0432 - A  10/22/03 1:41 PM  Page 26')]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieved_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "391de685",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAI\n",
    "llm = OpenAI(temperature=0.4, max_tokens=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ab374760",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "system_prompt = (\n",
    "    \"You are an assistant for question-answering tasks. \"\n",
    "    \"Use the following pieces of retrieved context to answer \"\n",
    "    \"the question. If you don't know the answer, say that you \"\n",
    "    \"don't know. Use three sentences maximum and keep the \"\n",
    "    \"answer concise.\"\n",
    "    \"\\n\\n\"\n",
    "    \"{context}\"\n",
    ")\n",
    "\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7b9891b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "question_answer_chain = create_stuff_documents_chain(llm, prompt)\n",
    "rag_chain = create_retrieval_chain(retriever, question_answer_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2c5db230",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0.4, max_tokens=500)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1be7338a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# System prompt and prompt template\n",
    "system_prompt = (\n",
    "    \"You are an assistant for question-answering tasks. \"\n",
    "    \"Use the following pieces of retrieved context to answer the question. \"\n",
    "    \"If you don't know the answer, say so. Be concise.\"\n",
    "    \"\\n\\n\"\n",
    "    \"{context}\"\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", system_prompt),\n",
    "    (\"human\", \"{input}\"),\n",
    "])\n",
    "\n",
    "# Make sure you are using the 3.5 model here:\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0.4, max_tokens=500)\n",
    "\n",
    "# Create chain properly using the same LLM\n",
    "question_answer_chain = create_stuff_documents_chain(llm, prompt)\n",
    "\n",
    "# Assuming 'retriever' is already set\n",
    "rag_chain = create_retrieval_chain(retriever, question_answer_chain)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5e737fca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GALE ENCYCLOPEDIA OF MEDICINE 226\n",
      "Acne\n",
      "GEM - 0001 to 0432 - A  10/22/03 1:41 PM  Page 26\n"
     ]
    }
   ],
   "source": [
    "docs = retriever.invoke(\"what is Acne?\")\n",
    "print(docs[0].page_content if docs else \"No docs found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8bb461e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "54\n"
     ]
    }
   ],
   "source": [
    "print(len(docs))  # Number of documents retrieved\n",
    "print(sum(len(d.page_content.split()) for d in docs))  # Approx total words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4403f64e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b6b3633e",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo-16k\", temperature=0.4, max_tokens=1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "47615167",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: /Users/apple/Desktop/End-to-end-Medical-Chatbot-Generative-AI\n",
      "Looking for 'Data' directory at: /Users/apple/Desktop/Data\n",
      "'Data' directory does not exist at /Users/apple/Desktop/Data\n",
      "File not found: ../Data/Medical_book.pdf. Please check the path and ensure the file exists.\n"
     ]
    }
   ],
   "source": [
    "# Path to your PDF\n",
    "file_path = \"../Data/Medical_book.pdf\"\n",
    "data_dir = \"../Data\"\n",
    "\n",
    "# Debug: Print current working directory and list files in the actual data directory\n",
    "print(\"Current working directory:\", os.getcwd())\n",
    "print(\"Looking for 'Data' directory at:\", os.path.abspath(data_dir))\n",
    "if os.path.exists(data_dir):\n",
    "    print(\"Files in '../Data':\", os.listdir(data_dir))\n",
    "else:\n",
    "    print(\"'Data' directory does not exist at\", os.path.abspath(data_dir))\n",
    "\n",
    "# Check if file exists\n",
    "if not os.path.exists(file_path):\n",
    "    print(f\"File not found: {file_path}. Please check the path and ensure the file exists.\")\n",
    "else:\n",
    "    # Load PDF\n",
    "    loader = PyPDFLoader(file_path)\n",
    "    docs = loader.load()\n",
    "\n",
    "    # Split into chunks for better retrieval\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\n",
    "    chunks = splitter.split_documents(docs)\n",
    "\n",
    "    # Generate embeddings\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "    # Create FAISS vectorstore\n",
    "    vectorstore = FAISS.from_documents(chunks, embeddings)\n",
    "    print(f\"FAISS vectorstore created with {len(chunks)} chunks.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9f7d7995",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3a883867",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the existing retriever and llm\n",
    "qa_chain = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3f2cb9e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = docsearch.as_retriever(search_type=\"similarity\", search_kwargs={\"k\":1})\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=retriever\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "04ad61f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API Key Set: True\n",
      "Vectorstore Exists: True\n",
      "QA Chain Exists: True\n"
     ]
    }
   ],
   "source": [
    "print(\"API Key Set:\", \"OPENAI_API_KEY\" in os.environ)\n",
    "print(\"Vectorstore Exists:\", 'vectorstore' in globals())\n",
    "print(\"QA Chain Exists:\", 'qa_chain' in globals())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0e88ffd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: /Users/apple/Desktop/End-to-end-Medical-Chatbot-Generative-AI\n",
      "Looking for 'Data' directory at: /Users/apple/Desktop/End-to-end-Medical-Chatbot-Generative-AI/Data\n",
      "Files in 'Data': ['Medical_book.pdf']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device_name: mps\n",
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ FAISS vectorstore created with 6600 chunks.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "import os\n",
    "\n",
    "file_path = \"Data/Medical_book.pdf\"\n",
    "data_dir = \"Data\"\n",
    "\n",
    "print(\"Current working directory:\", os.getcwd())\n",
    "print(\"Looking for 'Data' directory at:\", os.path.abspath(data_dir))\n",
    "if os.path.exists(data_dir):\n",
    "    print(\"Files in 'Data':\", os.listdir(data_dir))\n",
    "else:\n",
    "    print(\"'Data' directory does not exist at\", os.path.abspath(data_dir))\n",
    "\n",
    "if not os.path.exists(file_path):\n",
    "    print(f\"File not found: {file_path}. Please check the path and ensure the file exists.\")\n",
    "else:\n",
    "    loader = PyPDFLoader(file_path)\n",
    "    docs = loader.load()\n",
    "\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\n",
    "    chunks = splitter.split_documents(docs)\n",
    "\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "    vectorstore = FAISS.from_documents(chunks, embeddings)\n",
    "    print(f\"‚úÖ FAISS vectorstore created with {len(chunks)} chunks.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "01c93f43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ QA Chain is ready!\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
    "retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\":1})\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=retriever\n",
    ")\n",
    "\n",
    "print(\"‚úÖ QA Chain is ready!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ec186e2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: hf_xet in /Users/apple/miniconda3/envs/medibot/lib/python3.10/site-packages (1.1.10)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install hf_xet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "73b144a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /Users/apple/miniconda3/envs/medibot/lib/python3.10/site-packages (4.51.3)\n",
      "Requirement already satisfied: torch in /Users/apple/miniconda3/envs/medibot/lib/python3.10/site-packages (2.6.0)\n",
      "Requirement already satisfied: accelerate in /Users/apple/miniconda3/envs/medibot/lib/python3.10/site-packages (1.10.1)\n",
      "Requirement already satisfied: bitsandbytes in /Users/apple/miniconda3/envs/medibot/lib/python3.10/site-packages (0.42.0)\n",
      "Requirement already satisfied: filelock in /Users/apple/miniconda3/envs/medibot/lib/python3.10/site-packages (from transformers) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /Users/apple/miniconda3/envs/medibot/lib/python3.10/site-packages (from transformers) (0.30.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/apple/miniconda3/envs/medibot/lib/python3.10/site-packages (from transformers) (2.0.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/apple/miniconda3/envs/medibot/lib/python3.10/site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/apple/miniconda3/envs/medibot/lib/python3.10/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/apple/miniconda3/envs/medibot/lib/python3.10/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /Users/apple/miniconda3/envs/medibot/lib/python3.10/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /Users/apple/miniconda3/envs/medibot/lib/python3.10/site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /Users/apple/miniconda3/envs/medibot/lib/python3.10/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/apple/miniconda3/envs/medibot/lib/python3.10/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /Users/apple/miniconda3/envs/medibot/lib/python3.10/site-packages (from torch) (4.13.2)\n",
      "Requirement already satisfied: networkx in /Users/apple/miniconda3/envs/medibot/lib/python3.10/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /Users/apple/miniconda3/envs/medibot/lib/python3.10/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /Users/apple/miniconda3/envs/medibot/lib/python3.10/site-packages (from torch) (2025.3.2)\n",
      "Requirement already satisfied: sympy==1.13.1 in /Users/apple/miniconda3/envs/medibot/lib/python3.10/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/apple/miniconda3/envs/medibot/lib/python3.10/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: psutil in /Users/apple/miniconda3/envs/medibot/lib/python3.10/site-packages (from accelerate) (5.9.0)\n",
      "Requirement already satisfied: scipy in /Users/apple/miniconda3/envs/medibot/lib/python3.10/site-packages (from bitsandbytes) (1.15.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/apple/miniconda3/envs/medibot/lib/python3.10/site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/apple/miniconda3/envs/medibot/lib/python3.10/site-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/apple/miniconda3/envs/medibot/lib/python3.10/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/apple/miniconda3/envs/medibot/lib/python3.10/site-packages (from requests->transformers) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/apple/miniconda3/envs/medibot/lib/python3.10/site-packages (from requests->transformers) (2025.1.31)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "üöÄ Loading free medical model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n",
      "/var/folders/nl/q3f58lb91kl7sqnyg6y9r4dr0000gn/T/ipykernel_31102/782170015.py:32: LangChainDeprecationWarning: The class `HuggingFacePipeline` was deprecated in LangChain 0.0.37 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFacePipeline``.\n",
      "  llm = HuggingFacePipeline(pipeline=pipe)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Free local model loaded successfully!\n",
      "üß™ Testing with: 'What is Acne?'\n",
      "\n",
      "============================================================\n",
      "ü§ñ MEDICAL BOT RESPONSE:\n",
      "============================================================\n",
      "Use the following medical context to answer the question accurately and helpfully.\n",
      "\n",
      "Medical Context:\n",
      "Journal of Urology(Mar. 1998): 935-940.\n",
      "Nancy J. Nordenson\n",
      "Acid reflux see Heartburn\n",
      "Acidosis see Respiratory acidosis; Renal\n",
      "tubular acidosis; Metabolic acidosis\n",
      "Acne\n",
      "Definition\n",
      "Acne is a common skin disease characterized by\n",
      "pimples on the face, chest, and back. It occurs when the\n",
      "pores of the skin become clogged with oil, dead skin\n",
      "cells, and bacteria.\n",
      "Description\n",
      "Acne vulgaris, the medical term for common acne, is\n",
      "the most common skin disease. It affects nearly 17 million\n",
      "\n",
      "Question: What is Acne?\n",
      "\n",
      "Provide a clear, concise medical answer based only on the context above. If the context doesn't contain the answer, say \"I don't have enough medical information about this.\"\n",
      "\n",
      "============================================================\n",
      "\n",
      "üìö Retrieved 1 relevant medical documents\n",
      "\n",
      "Source 1:\n",
      "Page: 37\n",
      "Content: Journal of Urology(Mar. 1998): 935-940.\n",
      "Nancy J. Nordenson\n",
      "Acid reflux see Heartburn\n",
      "Acidosis see Respiratory acidosis; Renal\n",
      "tubular acidosis; Metabo...\n"
     ]
    }
   ],
   "source": [
    "%pip install transformers torch accelerate bitsandbytes\n",
    "\n",
    "from langchain_community.llms import HuggingFacePipeline\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM\n",
    "from langchain.prompts import PromptTemplate\n",
    "import os\n",
    "\n",
    "# Fix tokenizer warning\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "print(\"üöÄ Loading free medical model...\")\n",
    "\n",
    "# Use a small, fast model that's good for medical Q&A\n",
    "model_name = \"microsoft/DialoGPT-medium\"  # Free, fast, good for conversations\n",
    "\n",
    "try:\n",
    "    # Load model and tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "    \n",
    "    # Create pipeline\n",
    "    pipe = pipeline(\n",
    "        \"text-generation\",\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        max_new_tokens=150,\n",
    "        temperature=0.3,\n",
    "        do_sample=True,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    \n",
    "    llm = HuggingFacePipeline(pipeline=pipe)\n",
    "    \n",
    "    # Custom prompt for medical Q&A\n",
    "    prompt_template = \"\"\"Use the following medical context to answer the question accurately and helpfully.\n",
    "\n",
    "Medical Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Provide a clear, concise medical answer based only on the context above. If the context doesn't contain the answer, say \"I don't have enough medical information about this.\"\n",
    "\"\"\"\n",
    "\n",
    "    PROMPT = PromptTemplate(\n",
    "        template=prompt_template, input_variables=[\"context\", \"question\"]\n",
    "    )\n",
    "    \n",
    "    # Update your QA chain with better prompt\n",
    "    qa_chain = RetrievalQA.from_chain_type(\n",
    "        llm=llm,\n",
    "        chain_type=\"stuff\",\n",
    "        retriever=retriever,\n",
    "        chain_type_kwargs={\"prompt\": PROMPT},\n",
    "        return_source_documents=True\n",
    "    )\n",
    "    \n",
    "    print(\"‚úÖ Free local model loaded successfully!\")\n",
    "    \n",
    "    # Test it immediately\n",
    "    query = \"What is Acne?\"\n",
    "    print(f\"üß™ Testing with: '{query}'\")\n",
    "    \n",
    "    response = qa_chain.invoke({\"query\": query})\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"ü§ñ MEDICAL BOT RESPONSE:\")\n",
    "    print(\"=\"*60)\n",
    "    print(response['result'])\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Show source information\n",
    "    print(f\"\\nüìö Retrieved {len(response['source_documents'])} relevant medical documents\")\n",
    "    for i, doc in enumerate(response['source_documents'][:2]):\n",
    "        print(f\"\\nSource {i+1}:\")\n",
    "        print(f\"Page: {doc.metadata.get('page', 'N/A')}\")\n",
    "        print(f\"Content: {doc.page_content[:150]}...\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error loading model: {e}\")\n",
    "    print(\"Trying an even simpler approach...\")\n",
    "    \n",
    "    # Fallback: Direct retrieval + generation\n",
    "    def ask_medical_question(question):\n",
    "        docs = retriever.invoke(question)\n",
    "        context = \"\\n\".join([doc.page_content for doc in docs[:3]])\n",
    "        \n",
    "        prompt = f\"\"\"Based on this medical information:\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Please provide a clear answer:\"\"\"\n",
    "        \n",
    "        response = llm.invoke(prompt)\n",
    "        return response, docs\n",
    "    \n",
    "    # Test fallback\n",
    "    query = \"What is Acne?\"\n",
    "    answer, sources = ask_medical_question(query)\n",
    "    print(\"\\nü§ñ Medical Answer (Fallback):\")\n",
    "    print(answer)\n",
    "    print(f\"\\nüìö Retrieved {len(sources)} relevant documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5868aee2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: streamlit in /Users/apple/miniconda3/envs/medibot/lib/python3.10/site-packages (1.50.0)\n",
      "Requirement already satisfied: altair!=5.4.0,!=5.4.1,<6,>=4.0 in /Users/apple/miniconda3/envs/medibot/lib/python3.10/site-packages (from streamlit) (5.5.0)\n",
      "Requirement already satisfied: blinker<2,>=1.5.0 in /Users/apple/miniconda3/envs/medibot/lib/python3.10/site-packages (from streamlit) (1.9.0)\n",
      "Requirement already satisfied: cachetools<7,>=4.0 in /Users/apple/miniconda3/envs/medibot/lib/python3.10/site-packages (from streamlit) (6.2.0)\n",
      "Requirement already satisfied: click<9,>=7.0 in /Users/apple/miniconda3/envs/medibot/lib/python3.10/site-packages (from streamlit) (8.1.8)\n",
      "Requirement already satisfied: numpy<3,>=1.23 in /Users/apple/miniconda3/envs/medibot/lib/python3.10/site-packages (from streamlit) (2.0.1)\n",
      "Requirement already satisfied: packaging<26,>=20 in /Users/apple/miniconda3/envs/medibot/lib/python3.10/site-packages (from streamlit) (24.2)\n",
      "Requirement already satisfied: pandas<3,>=1.4.0 in /Users/apple/miniconda3/envs/medibot/lib/python3.10/site-packages (from streamlit) (2.3.3)\n",
      "Requirement already satisfied: pillow<12,>=7.1.0 in /Users/apple/miniconda3/envs/medibot/lib/python3.10/site-packages (from streamlit) (11.2.1)\n",
      "Requirement already satisfied: protobuf<7,>=3.20 in /Users/apple/miniconda3/envs/medibot/lib/python3.10/site-packages (from streamlit) (5.29.4)\n",
      "Requirement already satisfied: pyarrow>=7.0 in /Users/apple/miniconda3/envs/medibot/lib/python3.10/site-packages (from streamlit) (21.0.0)\n",
      "Requirement already satisfied: requests<3,>=2.27 in /Users/apple/miniconda3/envs/medibot/lib/python3.10/site-packages (from streamlit) (2.32.3)\n",
      "Requirement already satisfied: tenacity<10,>=8.1.0 in /Users/apple/miniconda3/envs/medibot/lib/python3.10/site-packages (from streamlit) (9.1.2)\n",
      "Requirement already satisfied: toml<2,>=0.10.1 in /Users/apple/miniconda3/envs/medibot/lib/python3.10/site-packages (from streamlit) (0.10.2)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.4.0 in /Users/apple/miniconda3/envs/medibot/lib/python3.10/site-packages (from streamlit) (4.13.2)\n",
      "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /Users/apple/miniconda3/envs/medibot/lib/python3.10/site-packages (from streamlit) (3.1.45)\n",
      "Requirement already satisfied: pydeck<1,>=0.8.0b4 in /Users/apple/miniconda3/envs/medibot/lib/python3.10/site-packages (from streamlit) (0.9.1)\n",
      "Requirement already satisfied: tornado!=6.5.0,<7,>=6.0.3 in /Users/apple/miniconda3/envs/medibot/lib/python3.10/site-packages (from streamlit) (6.4.2)\n",
      "Requirement already satisfied: jinja2 in /Users/apple/miniconda3/envs/medibot/lib/python3.10/site-packages (from altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (3.1.6)\n",
      "Requirement already satisfied: jsonschema>=3.0 in /Users/apple/miniconda3/envs/medibot/lib/python3.10/site-packages (from altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (4.25.1)\n",
      "Requirement already satisfied: narwhals>=1.14.2 in /Users/apple/miniconda3/envs/medibot/lib/python3.10/site-packages (from altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (2.7.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /Users/apple/miniconda3/envs/medibot/lib/python3.10/site-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.12)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/apple/miniconda3/envs/medibot/lib/python3.10/site-packages (from pandas<3,>=1.4.0->streamlit) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/apple/miniconda3/envs/medibot/lib/python3.10/site-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/apple/miniconda3/envs/medibot/lib/python3.10/site-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/apple/miniconda3/envs/medibot/lib/python3.10/site-packages (from requests<3,>=2.27->streamlit) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/apple/miniconda3/envs/medibot/lib/python3.10/site-packages (from requests<3,>=2.27->streamlit) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/apple/miniconda3/envs/medibot/lib/python3.10/site-packages (from requests<3,>=2.27->streamlit) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/apple/miniconda3/envs/medibot/lib/python3.10/site-packages (from requests<3,>=2.27->streamlit) (2025.1.31)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /Users/apple/miniconda3/envs/medibot/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/apple/miniconda3/envs/medibot/lib/python3.10/site-packages (from jinja2->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (3.0.2)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /Users/apple/miniconda3/envs/medibot/lib/python3.10/site-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (25.3.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /Users/apple/miniconda3/envs/medibot/lib/python3.10/site-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (2025.9.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /Users/apple/miniconda3/envs/medibot/lib/python3.10/site-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (0.36.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /Users/apple/miniconda3/envs/medibot/lib/python3.10/site-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (0.27.1)\n",
      "Requirement already satisfied: six>=1.5 in /Users/apple/miniconda3/envs/medibot/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install streamlit\n",
    "\n",
    "# Create a new file: medical_chatbot_app.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "bbe0bb20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ FAISS index saved as 'faiss_index' folder\n"
     ]
    }
   ],
   "source": [
    "# Save the FAISS index for the Streamlit app\n",
    "vectorstore.save_local(\"faiss_index\")\n",
    "print(\"‚úÖ FAISS index saved as 'faiss_index' folder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "495643e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ FAISS index folder exists\n",
      "üìÅ Files in faiss_index: ['index.faiss', 'index.pkl']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "# Check if FAISS index exists\n",
    "if os.path.exists(\"faiss_index\"):\n",
    "    print(\"‚úÖ FAISS index folder exists\")\n",
    "    files = os.listdir(\"faiss_index\")\n",
    "    print(f\"üìÅ Files in faiss_index: {files}\")\n",
    "else:\n",
    "    print(\"‚ùå FAISS index folder not found\")\n",
    "    print(\"Please run the notebook to create the FAISS index first\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "medibot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
